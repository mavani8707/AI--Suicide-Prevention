<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="styles.css" />

    <title>Risks</title>
  </head>
  <body>
    <nav class="navbar">
      <ul class="nav-links">
        <li><a href="index.html">Home page</a></li>
        <li><a href="topic.html">Technology/Topic</a></li>
        <li><a href="opportunities.html">Opportunities</a></li>
        <li><a href="risks.html">Risks</a></li>
        <li><a href="choices.html">Choices</a></li>
        <li><a href="references.html">References</a></li>
        <li class="dropdown">
          <a href="#">Process Support</a>
          <ul class="dropdown-content">
            <li><a href="teamformation.html">Team Formation</a></li>
            <li><a href="topic_proposal.html">Topic Proposal</a></li>
            <li><a href="meetingminutes.html">Meeting Minutes</a></li>
            <li><a href="projectportfolio.html">Project Portfolio</a></li>
            <li><a href="assessment_rubric.html">Assessment Rubric</a></li>
            <li><a href="guideline_conformance.html">Guideline Conformance</a></li>
          </ul>
        </li>
      </ul>
    </nav>
    <div class="container">
      <h1 class="text-center">Risks of the use of AI powered tools and Iniitiatives for suicide prevention</h1>
      <img src="Images/ai_mental_health_limitations.png" width="35%" style="width: 450px; height: 400px; display: block; margin: 0 auto;alt="Fig 5: Current Limitations in Mental Healthcare"> 
<p
        style="
          display: block;
          margin: 0 auto;
          text-align: center;
          font-size: small;
        "
      >
        Fig 5: Current Limitations in Mental Healthcare 
      </p>
      <p style="display: block; margin: 0 auto; text-align: center; font-size: small;"><a href="https://www.xcode.life/genes-and-ai/the-important-role-of-ai-in-mental-health-research">Image Source</a></p>
      <p>Prevention of suicide is a global health priority, with "approximately 800,000 individuals dying by suicide yearly, and for every suicide death, there are another 20 estimated suicide attempts” (Holmes et al., 2025). With recent development, AI plays a major role as they are more scalable, affordable and accessible. In addition, they can detect subtle data patterns, relationships and variable interactions that are difficult for humans to discern. However, there are numerous risks involved with integration of AI in such a sensitive field, which raise ethical, privacy and practical application challenges in AI-based models. This part will critically examine the risk by carefully considering recent research to show undeniable risk and limitations involved with implementing AI powered tools and initiatives for suicide prevention.</p>

		<p>The rising suicide rates all over the world annually have led to the adoption of AI, which has acted as a catalyst in advancing suicide prevention. However, the rise of AI use is directly proportional to the risk involved with it. Therefore, we have identified key risks, which are critical and play a significant role in people’s lives.</p>

		<p>In today’s technological era, people generate an estimated 402.74 million terabytes of data every day (Exploding Topics, 2025), which comes from every possible sector, whether it’s social media, health records or day-to-day online interactions. This massive data production highlights the critical need to value and protect the “privacy, security and anonymity of both the original posters and end users” (Holmes et al.,2025). As AI models are often trained on such sensitive data, which intensifies ethical and security concerns. Therefore, when considering AI-powered suicide prevention tools, the collection, sharing and use of personal data without explicit consent raise a growing area of concern that must be critically examined. Recent studies have identified that the integration of AI and social media data acts as a double-edged sword in terms of balancing privacy and confidentiality concerning the use of public data with opportunities for suicide detection. It was found that only 1 of the 31 identified studies sought such permissions (Holmes et al., 2025). Moreover, it was found that users of these social media platforms were barely informed about the consequences, and they weren’t given the opportunity to opt out of this analysis. Which, on the other hand, directly increases the risk of privacy breaches and undermining fundamental rights (Abdelmoteleb et al.,2025). In addition, this concern violates the right to privacy. Therefore, legislative measures, for instance, “recently underscored by European legislation prohibiting certain data-gathering practices related to health information by the algorithms of Facebook and other large social media companies” (European Commission,2022). Furthermore, studies note significant bias in AI models related to demographics such as religion, race, nationality, sexuality and age, which is potentially disadvantageous to vulnerable groups (Homes et al.,2025).</p>
<img src="Images/global-data-generated-annually.WEBP" width="60%" style="display: block; margin: 0 auto;" alt="Figure 6: Statistics for Global Data generated annually "> 
<p
        style="
          display: block;
          margin: 0 auto;
          text-align: center;
          font-size: small;
        "
      >
        Figure 6: Statistics for Global Data generated annually  
      </p>
      <p style="display: block; margin: 0 auto; text-align: center; font-size: small;"><a href="https://explodingtopics.com/blog/data-generated-per-day">Image Source</a></p>
		<p>The Lee-Luda incident is a reminder of how the integration of AI in the real world without proper monitoring guidance can cause disastrous events. The ScatterLab app’s parent company was found collecting intimate conversations between lovers without informing the user and using them for creating AI chatbot called Lee-Luda. Nowadays, most people are not confident about how their data will be used by the company, and it can be hard to imagine how it can cause potential harm; therefore, a recent incident of South Korea data misuse provides us with a clear picture. Interestingly, Lee-Luda gained popularity quickly, having around 750,000 user conversations within the first couple of weeks. However, within 2 weeks of launch, people started questioning whether the data was refined enough, as it started using verbally abusive language about certain social groups and sexually explicit comments to several users. </p>
		<p>It becomes clearer when it started revealing people’s nicknames and home addresses in its responses. It was found that prior to Lee-Luda’s release, the company uploaded around 1700 sentences, which were part of a large dataset it collected, on GitHub. GitHub is an open-source platform that developers use to store and share code and data. Furthermore, it was found the GitHub training dataset exposed names of more than 20 people, along with their relationship status, the location they have been to and some of their medical information. While this incident was a big story in South Korea, it received very little attention elsewhere. However, this incident highlights the general trend of AI industry, where people have little control over their personal data and how it is used and processed once it’s collected. In fact, it took five years for users to recognize that their personal data was being used to train the data model without their consent. Neither did they know that ScatterLab shared their private conversations on an open-source platform like GitHub, where anyone can gain access (Jang,2021).</p>
		<p>Recent studies found that despite incorporating psychological and advanced statistical modelling, they fail to demonstrate actionable accuracy, especially over short-term intervals. Moreover, it was not just about poor sensitivity or low positive predictive value but the absence of robust, targeted interventions that can be meaningfully used based on risk stratification. Therefore, without clear evidence about which intervention works for which risk profiles, the use of these tools is less beneficial (Abdelmoteleb et al., 2025). It is important that predictive models should complement rather than replace clinical judgment and human interaction. Over-reliance on these models without considering patient nuances and clinical expertise could harm patient health (atmakuru et al., 2025). Despite considering AI’s ability to detect suicide risk, there is still a drawback regarding their ability to identify individual risk trajectories or precise timing of predicting suicide crisis remains limited. Moreover, current models are highly focused on who questions rather than when predictions, which is a distinction important for clinical implementation (Abdelmoteleb et al., 2025). Lastly, it is important that AI models learn and adapt based on specific subgroups or religions to ensure its effectiveness in different cultural and societal contexts (atmakuru et al., 2025).</p>
		<p>Adaptive learning systems approach is to collect data from a student to personalise their educational experience, its appeal is that it creates a definite automated decision without human intervention. It is believed that the data-driven algorithmic system will produce a decision which is “inherently objective and fair,” (Karumbaiah, 2022). This comes at the expense of extensive data collection. Large amounts of data collected in a system have the risk of misuse of data and data breaches. Educational institutes are often a target of cyberattacks as they often have poor cybersecurity and contain personal information about students. Without robust cybersecurity, introducing adaptive learning systems could prove to be dangerous and put students and staff at risk.</p>
		<p>In conclusion, adaptive learning systems in education offer a wide range of opportunities for educational enhancement, and also introduces risks which must be investigated further. Adaptive learning systems rely heavily on collecting personal data from students, if not developed accurately, concerns about bias and privacy are raised. Bias can come from many different forms, data bias, algorithmic bias, and interaction bias, and have the potential to reinforce inequalities and stereotypes. Furthermore, the reliance on data collection brings the risk of cybersecurity threats, and so highlights the lack of policies to protect personal data. The use of adaptive learning systems creates a shift towards a greater time spent learning online/remotely. This raises the risk of creating a greater digital divide, where there is unequal access to devices and the internet, creating the possibility of exclusion in education. By evaluating the risks, and the implications they have on individuals and society, we can develop strategies to ensure adaptive learning systems are implemented and used ethically.</p>
	</div>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
      crossorigin="anonymous"
    ></script>
  </body>
</html>