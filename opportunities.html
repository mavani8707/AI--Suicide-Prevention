<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="styles.css" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="styles.css" />

    <title>Opportunities</title>
  </head>
  <body>
    <nav class="navbar">
      <ul class="nav-links">
        <li><a href="index.html">Home page</a></li>
        <li><a href="topic.html">Technology/Topic</a></li>
        <li><a href="opportunities.html">Opportunities</a></li>
        <li><a href="risks.html">Risks</a></li>
        <li><a href="choices.html">Choices</a></li>
        <li><a href="references.html">References</a></li>
        <li class="dropdown">
          <a href="#">Process Support</a>
          <ul class="dropdown-content">
            <li><a href="teamformation.html">Team Formation</a></li>
            <li><a href="topic_proposal.html">Topic Proposal</a></li>
            <li><a href="meetingminutes.html">Meeting Minutes</a></li>
            <li><a href="projectportfolio.html">Project Portfolio</a></li>
            <li><a href="assessment_rubric.html">Assessment Rubric</a></li>
            <li><a href="guideline_conformance.html">Guideline Conformance</a></li>
          </ul>
        </li>
      </ul>
    </nav>
    <div class="container">
      <h1 class="text-center">Opportunities of the Use of AI Powered Tools and Initiatives for Suicide Prevention </h1>


      <img src="Images/REACH_VET.PNG" width="30%" alt="Logo for REACH VET (Recovery Engagement and Coordination for Health – Veterans Enhanced Treatment)" style="display: block; margin: 0 auto;"> 

<p
        style="
          display: block;
          margin: 0 auto;
          text-align: center;
          font-size: small;
        "
      >
        Fig. 3: Logo for REACH VET (Recovery Engagement and Coordination for Health)
      </p>
      <p style="display: block; margin: 0 auto; text-align: center; font-size: small;"><a href="https://iava.org/media/read-reach-vet-reaching-a-solution ">Image Source</a></p>



      <p>Mental health services across our world are stretched thin. At-risk individuals attempting to seek help face long waiting times, limited access to professionals, or live in areas where there is no access to these resources at all (Greaves, 2024). However, AI powered tools provide us with the opportunity to overcome these limitations by providing low cost, fast, accurate, available and accessible healthcare (Hotman, 2020). Suicide prevention goes hand-in-hand with prediction, and AI brings about new and transformative opportunities to support this effort. There are two types of AI tools used in the context of suicide prevention/prediction; social and medical (Hotman, 2020).</p>
	  <p>There are many benefits to the use of AI for suicide prevention, one of the opportunities it brings is real-time personalization and support. Many individuals view seeking help as burdensome, they’re afraid of being judged, medicated or hospitalized (Hotman, 2020).  A study on the experiences of 13 UK-based crisis service volunteers tells us that individuals, when seeking help feel as though they’re imposing their problems on others, often apologising. (Greaves, 2024). Fear of seeking help is a serious problem especially when it comes to suicide ideation, it prevents people from getting the support that they need. AI can be used to overcome this in the form of conversational agents who use NPL (Natural language processing) to simulate real life, guilt-free conversations (Fonseka et al, 2019). These are available 24/7, with fast responses and no waiting lists. Large language models such as Open AI’s Chat GPT can deliver near human performance, and in some cases can outperform traditional systems due to their consistency with best practice. This helps reserve human support for those that desire it. It also lifts the stress of burdening someone with your problems and eliminates the fear of judgement, helping individuals be more honest (Greaves, 2024). Some conversational agents also have added features such as facial expressions and body language analyzation to signal when human interaction in necessary (Fonseka et al, 2019).</p>
    <img src="Images/10.1177_0004867419864428-fig2.png" width="60%" style="display: block; margin: 0 auto;"alt="Fig. 4: Graphic of how information is collected to be analysed by AI models to generate predictive and preventive solutions."> 
<p
        style="
          display: block;
          margin: 0 auto;
          text-align: center;
          font-size: small;
        "
      >
        Fig. 4: Graphic of how information is collected to be analysed by AI models to generate predictive and preventive solutions. 
      </p>

    <p style="display: block; margin: 0 auto; text-align: center; font-size: small;"><a href="https://journals.sagepub.com/doi/10.1177/0004867419864428">Image Source</a></p>
      <p>“As of February 2025 ... 5.24 billion, or 63.9% of the world's population, were social media users.” (Statistica, 2025). With so much of our world constantly interacting with digital spaces, integrating AI into social media platforms as tool for mental healthcare is an opportunity for early intervention, wider reach, and more accessible support. AI algorithms in sites such as YouTube, Instagram, Twitter, Facebook, etc, constantly monitor and remove self-injury and suicide-related content to prevent harm. This is important as evidence suggests that publicizing suicide without caution can increase risk of imitation and/or normalization. However, it is important to consider that AI often lacks the nuanced understanding needed to distinguish the seriousness of such expressions. In addition, some platforms also utilize AI conversational agents which “piggyback off the sites’ instant messaging interface" to interact with users about their interests, mood, etc to analyse their behavioural patterns and provide suggestions for evidence-based tools to help them (e.g. Facebook) (Fonseka et al, 2019).  </p>
      <p>The use of AI in above examples are social tools offering quick support for individuals (Hotman, 2020). Beyond individual use, AI in the form of medical suicide prediction tools can help us identify wider at-risk groups which offers opportunities such as policy reform and mobilizing resources for these populations (Fonseka et al, 2019). Despite decades of research in suicide prevention, it hasn’t improved in the past 50 years, AI offers a new opportunity for early detection of these risks on a larger scale (Hotman, 2020). “Machine Learning and AI have been used to generate predictive algorithms that can determine the effects of risk (and protective) factors on suicide outcomes, predict suicide outbreaks and identify at-risk populations.” (Fonseka et al, 2019). These algorithms are layered mathematical models which collect data from various sources such as Electronic Medical Records (EMRs), social media, wearable devices, internet search patterns, clinical notes and much more (Hotman, 2020). The data analysed by these algorithms can be used to generate predictive models that complement the currently limited mental health resources by becoming decision supporting tools (Hotman, 2020). One successful example is US’s REACH VET program, which offers support and optional psychological consult for veterans identified as at risk. Within just the first year of its implementation, there was a 4% reduction in suicides (250 less) “than what would have been expected from previous years” (Hotman, 2020). By converting complex data into actionable insights, these AI models provide us with the opportunities to make suicide prevention efforts more scalable allowing support to reach larger populations and far beyond what human-led systems have achieved. Though it is worth mentioning that ‘going off the book’ of such models, e.g. hospitalizing or forcefully providing support can put even more psychological pressure on the individuals. </p>
      <pIt is also important to understand that the research collected thus far on this topic has been with WEIRD populations (Western, Educated, Industrialized, Rich and Democratic), meaning that these results may not be generalizable to other demographics. Access to AI tools is heavily reliant on resources such as internet and devices which are not universally available, cultural differences may also affect the understanding and/or response to these tools, making them irrelevant to some. </p>
	  <p>Taking these benefits into consideration, we believe the opportunity of AI powered tools for suicide prevention lies in its ability to assist and complement rather than replace existing human-led systems and resources, helping fill in the gaps and enhancing the reach and response of existing mental health sources. 

 </p>
  </div>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
