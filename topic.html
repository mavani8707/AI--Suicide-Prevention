<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="styles.css" />

    <title>Topic</title>
  </head>
  <body>
    <nav class="navbar">
      <ul class="nav-links">
        <li><a href="index.html">Home page</a></li>
        <li><a href="topic.html">Technology/Topic</a></li>
        <li><a href="opportunities.html">Opportunities</a></li>
        <li><a href="risks.html">Risks</a></li>
        <li><a href="choices.html">Choices</a></li>
        <li><a href="references.html">References</a></li>
        <li class="dropdown">
          <a href="#">Process Support</a>
          <ul class="dropdown-content">
            <li><a href="teamformation.html">Team Formation</a></li>
            <li><a href="topic_proposal.html">Topic Proposal</a></li>
            <li><a href="meetingminutes.html">Meeting Minutes</a></li>
            <li><a href="projectportfolio.html">Project Portfolio</a></li>
            <li><a href="assessment_rubric.html">Assessment Rubric</a></li>
            <li>
              <a href="guideline_conformance.html">Guideline Conformance</a>
            </li>
          </ul>
        </li>
      </ul>
    </nav>
    <div class="container">
      <h1 class="text-center">AI-Powered Tools and Initiatives for Suicide Prevention</h1>
      <h2 class="text-center">Topic Description</h2>
      <p>
       Every 40 seconds, someone takes their own life (Ghebreyesus, 2019). But what if AI could change this statistic? Suicide rates are rising globally, with more than 800,000 people dying by suicide every year (refer to figure 2 for rates). This is a significant threat to people, as suicide leaves a damaging impact on society. AI offers new ways to detect suicide risks, such as wearable devices, social media and mobile apps that analyse this data (Bernert, 2020). This can be done through AI tools such as machine learning (ML) and natural language processing (NLP) (Berrouiguet, 2019). Machine learning is a subset of AI that enables computers to ‘learn’ from data, imitating the way that humans learn. NPL is a subfield of AI that uses machine learning to allow computers to communicate to humans.  (Holdsworth, 2024). These tools are used to look through data to create algorithms and visualize the results on suicide rates (Fonseka, 2019). This website will explore how AI is transforming suicide by identifying at-risk individuals, before it is too late.  
      </p>
      <p>
        AI offers hopeful opportunities in suicide prevention, by giving services such as real time support and personalised interventions. Tools such as chatbots, social media monitoring systems and predictive algorithms exhibit the potential to detect suicide risk faster and more accurately than medical experts. These technologies can improve effectiveness in mental health services, reduce guilt-free interactions, and allow for early intervention. However, this is followed by ethical challenges related to privacy, consent and transparency. The tools should establish a safe and trustworthy connection with the users, ensuring clear communication, responsible data practices and robust security measures. Facebook and Google have started to include these AI-driven systems in their platforms to identify at risk individuals. Nevertheless, there is limited evidence that supports the effectiveness of these systems. To provide safe resources to the user, more research needs to be conducted. For these tools to be implemented safely, there is needs to be clear policies set in place.   
      </p>
          
      
<p>
</p>

      <h2 class="text-center">Topic Justification</h2>
      <p>
        Using AI as conversational agents we can provide individuals with real time support, when human resources are less available. AI has also been found to detect possible suicide cases faster and with more accuracy than medical experts. Chatbots, social media and medical prediction tools have been used to achieve this purpose. This machinery, alongside the use of human resources have the capacity to improve overall efficiency (Miller DD, 2018). Furthermore, AI has been used to diagnose individuals and aid in daily monitoring of patients. The opportunities are plentiful, like real time personalisation and support, having guilt-free conversations and early intervention. In short, the use of AI shows clear advantages and benefits when it comes to prevent suicide (Bernert RA, 2020).
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
      <img src="Images/1suicide.jpg" width="60%" style="display: block; margin: 0 auto;"alt="Fig 2: It illustrates age-standardise rate of suicide  deaths per 100,000 population"> 
<p
        style="
          display: block;
          margin: 0 auto;
          text-align: center;
          font-size: small;
        "
      >
        Fig 2: It illustrates age-standardise rate of suicide  deaths per 100,000 population
      </p>
      <p style="display: block; margin: 0 auto; text-align: center; font-size: small;"><a href="https://www.parliament.nz/mi/pb/library-research-papers/research-papers/suicide-in-new-zealand-a-snapshot-of-recent-trends/">Image Source</a></p>      <p>
        
With any new technology there are risks that come with.   Using AI in mental health increases ethical problems that are connected to privacy, consent and transparency (Farhud, 2021). Another issue would be relating to the security AI brings, with things such as suicide prevention and algorithmic bias we need to consider to ensure responsibility of AI (Alowais et al. 2023). Considering these risks, there also needs to be proper communication between the companies and the patients. This fosters trust and it allows the users to make decisions leading to their lives being protected. By ensuring that consent has been given by the individual, the companies using AI can ensure that the users privacy is protected (Cohen, 2019). Companies such as Facebook, Google and even Apple have begun to execute in their apps and apply this to identify users that could be at risk (Lustgarten, 2021). However, with this recent emerging technology, there is not a lot of evidence provided about the usefulness and effectiveness of implementation in the healthcare field.      </p>
      <p>
In this field, AI can be used to navigate information and check through different habits from social media that are able to detect the risk of suicide, and they would be able to see their habits. Tools like AI are being evolved in research related to this field and are being developed by private technology companies. However, it is important that we use these tools responsibly and with a clear set of guidelines in place, so individuals can use them. AI needs to be used to support mental health interventions while respecting privacy, avoiding bias, and ensuring human oversight in decision making.
        
      </p>
<p>
Through this research and the answers given do propose the likelihood to increase suicide prevention using AI, the non-communication from big companies and the ethical implications show us the current difficulties given for Suicide prevention with the use of AI. 
</p>
    </div>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
